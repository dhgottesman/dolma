from .tokenizer import (
    Tokenizer,
    logger
)
import sys
import os
import gc
from .data_types import KASInputSpec, KASTokenizerOutput
from typing import TYPE_CHECKING, Generator, List, Tuple, Dict
from copy import deepcopy
import msgspec
import numpy as np
import smart_open

import spacy
nlp = spacy.load("en_core_web_sm")  # Load English model


ARTICLE_NAMESPACE = "Article"


class KASTokenizer(Tokenizer):
    def __init__(self, *args, **kwargs):
        self.mode = kwargs.pop("mode", "")
        super().__init__(*args, **kwargs)

    def encode(self, input: str, add_special_tokens: bool = True) -> List[int]:
        """
        Encode a string into token IDs.
        """
        encoding = self.base_tokenizer.encode(input, add_special_tokens=False)
        input_ids, offsets = encoding.ids, encoding.offsets
        
        if add_special_tokens:
            input_ids = self.add_special_tokens(input_ids)
        return input_ids, offsets

def make_tokenizer(
    tokenizer_name_or_path: str,
    **tokenizer_kwargs,
) -> KASTokenizer:
    tokenizer = (
        KASTokenizer.from_file(tokenizer_name_or_path, **tokenizer_kwargs)
        if os.path.exists(tokenizer_name_or_path) and os.path.isfile(tokenizer_name_or_path)
        else KASTokenizer.from_pretrained(tokenizer_name_or_path, **tokenizer_kwargs)
    )
    return tokenizer

def merge_entities(hyperlinks, coref, entity_linking):
    entities = []

    # Process hyperlinks
    for link in hyperlinks:
        entities.append({
            "char_start": link["start"],
            "char_end": link["end"],
            "text": link["surface_form"],
            "name": link["uri"].replace('_', ' '),
            "qcode": link.get("qcode", ""),
            "source": "hyperlink"
        })

    # Process coreference clusters
    for i, (cluster_texts, cluster_offsets) in enumerate(zip(coref["clusters_char_text"], coref["clusters_char_offsets"])):
        for text, (start, end) in zip(cluster_texts, cluster_offsets):
            entities.append({
                "char_start": start,
                "char_end": end,
                "text": text,
                "cluster_id": i,
                "source": "coreference"
            })

    # Process entity linking
    for link in entity_linking:
        link["char_start"] = link.pop("start")
        link["char_end"] = link["char_start"] + link.pop("ln")
        link["source"] = "entity_linking"
        entities.append(link)

    # Sort by start, then by end (shorter span first if starts are equal)
    entities.sort(key=lambda x: (x["char_start"], x["char_end"]))

    return entities

def add_token_indices(
    tokens: List[int],
    offsets: List[Tuple[int, int]],
    entities: List[Dict[str, int]],
    tokenizer
) -> Tuple[List[Dict[str, int]], List[int]]:
    """Finds token indexes for entities in a chunk of text."""
    entities = deepcopy(entities)
    entity_token_indexes = []
    tokenized_text = tokenizer.decode(tokens)

    for entity in entities:
        start_text = entity["char_start"]
        end_text = entity["char_end"]
        
        start_token = next(
            (i for i, (s, e) in enumerate(offsets) if s <= start_text < e), None
        )
        end_token = next(
            (i for i, (s, e) in enumerate(offsets) if s < end_text <= e), None
        )
        
        if start_token is None or end_token is None:
            continue
        
        entity.update({
            "tok_start": start_token,
            "tok_end": end_token + 1,  # Non-inclusive end
        })
        
        extracted_text = tokenized_text[start_text:end_text]
        decoded_tokens_text = tokenizer.decode(tokens[start_token:end_token + 1])
        
        assert extracted_text == entity["text"], f"Entity text mismatch: {extracted_text} != {entity['text']}"
        # if "ï¿½" not in decoded_tokens_text:
        assert extracted_text.strip() in decoded_tokens_text.strip(), f"Entity text mismatch: {extracted_text.strip()} not in {decoded_tokens_text.strip()}"
        
        entity_token_indexes.append(entity)
    
    return entity_token_indexes

def tokenize_file(
        tokenizer_name_or_path: str,
        path: str,
        refresh_tokenizer_every: int = 0,
        **tokenizer_kwargs,
    ) -> Generator[KASTokenizerOutput, None, None]:
        """Tokenize a file of documents using the provided tokenizer; file is expected to be a gzipped JSON lines
        file, each containing a field named `text`.
        """
        tokenizer = make_tokenizer(tokenizer_name_or_path, **tokenizer_kwargs)
        dtype = deepcopy(tokenizer.dtype)
        decoder = msgspec.json.Decoder(KASInputSpec)
        loc = 0
        with smart_open.open(path, mode="rt") as input_stream:
            for _, line in enumerate(input_stream, start=1):
                try:
                    row = decoder.decode(line)

                    if text := row.text.strip(): # Skip empty docs
                        # Union all entities
                        entities = merge_entities(row.hyperlinks, row.coref, row.entity_linking)

                        tokens, offsets = tokenizer.encode(text, add_special_tokens=True) # Daniela, should change to False
                        entities = add_token_indices(tokens, offsets, entities, tokenizer)
                        
                        if refresh_tokenizer_every:
                            # extra copy to prevent memory leaks
                            tokens = np.array(tokens, dtype=dtype)
                            yield KASTokenizerOutput.from_tokens(id=row.id, src=path, loc=loc, tokens=tokens, title=row.title, entities=entities, offsets=offsets) # pyright: ignore
                            loc += 1

                    if refresh_tokenizer_every > 0 and loc % refresh_tokenizer_every == 0:
                        # to prevent memory leaks, we refresh the tokenizer every so often
                        del tokenizer
                        gc.collect()
                        tokenizer = make_tokenizer(tokenizer_name_or_path, **tokenizer_kwargs)

                except Exception as ex:
                    logger.error("Error processing %s:%s", path, row.id, exc_info=ex)
    